from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
import torch
import time
import pandas as pd
import numpy as np
from typing import List, Dict
import re

class RussianGPT2SentimentAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä—É—Å—Å–∫–æ–π GPT-2"""
    
    def __init__(self, model_name='ai-forever/rugpt3small_based_on_gpt2'):
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # # –î–ª—è —Ä—É—Å—Å–∫–æ–π GPT-2 —á–∞—Å—Ç–æ –Ω—É–∂–µ–Ω padding token
        # if self.tokenizer.pad_token is None:
        #     self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
        print(time.ctime())
        print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def analyze_sentiment(self, text: str, prompt_template: str = "default",
                         temperature: float = 0.1, max_new_tokens: int = 10) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
        
        # –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        if len(text) > 200:
            text = text[:197] + "..."
        
        # –í—ã–±–æ—Ä —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞
        if prompt_template == "short":
            prompt = f"—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {text}"#f"–ö–ª–¢–µ–∫—Å—Ç: {text}\n–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞:"
        elif prompt_template == "medium":
            prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {text}"#f"–û—Ç–∑—ã–≤: '{text}'\–û—Ü–µ–Ω–∫–∞: >5 –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è <5 –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è =5 –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è:"
        elif prompt_template == "long":
            prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–≤—ã–±–µ—Ä–∏ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç):
–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π  
–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
–¢–µ–∫—Å—Ç:{text}"""
            # prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É. 
            # –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π.
            # –¢–µ–∫—Å—Ç: {text}
            # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:"""  
        elif prompt_template == "few_shot":
            # Few-shot —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
            prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–≤—ã–±–µ—Ä–∏ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç)
            –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π  
–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
            –ø—Ä–∏–º–µ—Ä—ã:
—Ç–µ–∫—Å—Ç: "–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!" -> –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
—Ç–µ–∫—Å—Ç: "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ –ø–æ–∫—É–ø–∞–π—Ç–µ" -> –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π
—Ç–µ–∫—Å—Ç: "–û–±—ã—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ" -> –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π

—Ç–µ–∫—Å—Ç: "{text}" ->"""            
# prompt = f"""–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞.
# –¢–µ–∫—Å—Ç: "–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!"
# –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π

# –¢–µ–∫—Å—Ç: "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–µ –ø–æ–∫—É–ø–∞–π—Ç–µ"
# –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π

# –¢–µ–∫—Å—Ç: "–û–±—ã—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ"
# –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π

# –¢–µ–ø–µ—Ä—å –æ–ø—Ä–µ–¥–µ–ª–∏:
# –¢–µ–∫—Å—Ç: "{text}"
# –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:"""
        else:  # default
            prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–≤—ã–±–µ—Ä–∏ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç):
–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π  
–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
–¢–µ–∫—Å—Ç:{text}"""
            
            #f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ '{text}': "
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True,
            truncation=True,
            max_length=256).to(self.device)
                
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,  # –ö–ª—é—á–µ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ!
                temperature=max(0.1, temperature),  # –ú–∏–Ω–∏–º—É–º 0.1
                do_sample=False, # temperature > 0.1,
                #pad_token_id=self.tokenizer.pad_token_id,
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                repetition_penalty=1.2
            )
        
        inference_time = time.time() - start_time
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
        generated_tokens = outputs[0]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        #print("__________–°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–û___________", generated_text)
        if prompt in generated_text:
           generated_text = generated_text.replace(prompt, "").strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑ –æ—Ç–≤–µ—Ç–∞
        sentiment = self._extract_sentiment(generated_text)
        
        return {
            'prompt': prompt,
            'generated_text': generated_text,
            'sentiment': sentiment,
            'inference_time': inference_time,
            'text_length': len(text),
            'response_length': len(generated_text)
        }
    
    def _extract_sentiment(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        positive_keywords = ['–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π', '–ø–æ–∑–∏—Ç–∏–≤', 'positive', '—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–∫–ª–∞—Å—Å–Ω', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω']
        negative_keywords = ['–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π', '–Ω–µ–≥–∞—Ç–∏–≤', 'negative', '–ø–ª–æ—Ö', '—É–∂–∞—Å', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω', '–∫–æ—à–º–∞—Ä–Ω']
        neutral_keywords = ['–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω', 'neutral', '—Å—Ä–µ–¥–Ω', '–æ–±—ã—á–Ω', '–Ω–æ—Ä–º–∞–ª—å–Ω', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤
        for word in positive_keywords:
            if word in text_lower:
                return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        
        for word in negative_keywords:
            if word in text_lower:
                return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        
        for word in neutral_keywords:
            if word in text_lower:
                return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - –ø—ã—Ç–∞–µ–º—Å—è —É–≥–∞–¥–∞—Ç—å –ø–æ –ø–µ—Ä–≤—ã–º —Å–∏–º–≤–æ–ª–∞–º
        first_words = text_lower.split()[:3]
        for word in first_words:
            if word.startswith('–ø–æ–ª') or word.startswith('pos') or word.startswith('–ø–æ–∑'):
                return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
            elif word.startswith('–æ—Ç—Ä') or word.startswith('neg') or word.startswith('–Ω–µ–≥'):
                return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
            elif word.startswith('–Ω–µ–π') or word.startswith('neu') :
                return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
        if any(word in text_lower for word in ['üëç', 'üòä', 'üòç', '‚ù§Ô∏è', '—Å—É–ø–µ—Ä', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ']):
            return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        elif any(word in text_lower for word in ['üëé', 'üò†', 'üò°', 'üíî', '—É–∂–∞—Å–Ω–æ', '–∫–æ—à–º–∞—Ä', '–ø–ª–æ—Ö–æ']):
            return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏
        if any(word in text_lower for word in ['5', '4', '–æ—Ç–ª–∏—á–Ω–æ', '—Ö–æ—Ä–æ—à–æ']):
            return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        elif any(word in text_lower for word in ['1', '2', '–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ']):
            return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        elif any(word in text_lower for word in ['3', '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '—Å—Ä–µ–¥–Ω–µ']):
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'

        
        return 'unknown'
    
    def batch_analyze(self, texts: List[str], prompt_template: str = "default",
                     temperature: float = 0.1) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        results = []
        inference_times = []
        
        print(f"  –ê–Ω–∞–ª–∏–∑ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        for i, text in enumerate(texts):
            try:
                result = self.analyze_sentiment(text, prompt_template, temperature)
                results.append(result)
                inference_times.append(result['inference_time'])
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if (i + 1) % 5 == 0:
                    print(f"    –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(texts)}")
                    
            except Exception as e:
                print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ {i}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª—É—à–∫—É
                results.append({
                    'prompt': '',
                    'generated_text': '',
                    'sentiment': 'unknown',
                    'inference_time': 0.0,
                    'text_length': len(text),
                    'response_length': 0
                })
                inference_times.append(0.0)
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = [r['sentiment'] for r in results]
        
        return {
            'predictions': predictions,
            'results': results,
            'avg_inference_time': np.mean(inference_times) if inference_times else 0,
            'avg_text_length': np.mean([r['text_length'] for r in results]) if results else 0,
            'avg_response_length': np.mean([r['response_length'] for r in results]) if results else 0
        }

def run_quick_experiments():
    """–ë—ã—Å—Ç—Ä—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (–¥–ª—è —Ç–µ—Å—Ç–∞)"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_df = pd.read_csv("data/rusentiment_train.csv")
    test_df['label'] = test_df['label'].apply(
    lambda x: '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π' if x == 'negative' else 
              ('–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π' if x == 'neutral' else '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'))
    print(test_df['label'].unique())
    test_texts = test_df['text'].tolist()[:20]  # –¢–æ–ª—å–∫–æ 20 –¥–ª—è —Ç–µ—Å—Ç–∞
    true_labels = test_df['label'].tolist()[:20]
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_texts)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    print("\n–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    analyzer = RussianGPT2SentimentAnalyzer()
    
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    print("\nüìù –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:")
    test_text = test_texts[10]
    true_label = true_labels[10]
    
    result = analyzer.analyze_sentiment(test_text, prompt_template="long")
    print(f"–¢–µ–∫—Å—Ç: {test_text[:50]}...")
    print(f"–ü—Ä–æ–º–ø—Ç: {result['prompt'][:50]}...")
    print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {result['generated_text']}")
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ: {result['sentiment']}")
    print(f"–†–µ–∞–ª—å–Ω–æ–µ: {true_label}")
    print(f"–í—Ä–µ–º—è: {result['inference_time']:.2f} —Å–µ–∫")
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    print("\n" + "="*60)
    print("üöÄ –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
    print("="*60)
    
    from sklearn.metrics import accuracy_score
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: –†–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã (–Ω–∞ 10 –ø—Ä–∏–º–µ—Ä–∞—Ö)
    print("\n1. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –†–∞–∑–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤")
    print("-"*40)
    
    templates = ['short', 'medium', 'long', 'few_shot']
    sample_size = 10
    
    for template in templates:
        print(f"\n  –®–∞–±–ª–æ–Ω: {template}")
        results = analyzer.batch_analyze(
            test_texts[:sample_size], 
            prompt_template=template
        )
        
        predictions = results['predictions']
        accuracy = accuracy_score(true_labels[:sample_size], predictions)
        
        print(f"    Accuracy: {accuracy:.1%}")
        print(f"    –í—Ä–µ–º—è: {results['avg_inference_time']:.3f} —Å–µ–∫/–ø—Ä–∏–º–µ—Ä")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        for i in range(min(2, len(results['results']))):
            r = results['results'][i]
            print(f"    –ü—Ä–∏–º–µ—Ä {i+1}: '{r['generated_text'][:30]}...' -> {r['sentiment']}")
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–Ω–∞ 5 –ø—Ä–∏–º–µ—Ä–∞—Ö)
    print("\n2. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –í–ª–∏—è–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã")
    print("-"*40)
    
    temperatures = [0.1, 0.7, 1.2]
    
    for temp in temperatures:
        print(f"\n  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temp}")
        
        predictions = []
        for text in test_texts[:5]:
            result = analyzer.analyze_sentiment(text, temperature=temp)
            predictions.append(result['sentiment'])
        
        accuracy = accuracy_score(true_labels[:5], predictions)
        print(f"    Accuracy: {accuracy:.1%}")
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
        unique_preds = set(predictions)
        print(f"    –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã: {unique_preds}")
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 3: Zero-shot vs Few-shot
    print("\n3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: Zero-shot vs Few-shot")
    print("-"*40)
    
    for template, name in [('medium', 'Zero-shot'), ('few_shot', 'long')]:
        print(f"\n  {name}:")
        results = analyzer.batch_analyze(test_texts[:8], prompt_template=template)
        predictions = results['predictions']
        #print("–ó–ö–£–í–´–õ–§–Ø",predictions[1])
        
        accuracy = accuracy_score(true_labels[:8], predictions)
        print(f"    Accuracy: {accuracy:.1%}")
        print(f"    –í—Ä–µ–º—è: {results['avg_inference_time']:.3f} —Å–µ–∫/–ø—Ä–∏–º–µ—Ä")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    print("\n" + "="*60)
    print("üìä –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –≤—Å–µ—Ö 20 –ø—Ä–∏–º–µ—Ä–∞—Ö —Å –ª—É—á—à–∏–º —à–∞–±–ª–æ–Ω–æ–º
    best_template = 'few_shot'
    print(f"\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à–∞–±–ª–æ–Ω–æ–º '{best_template}' –Ω–∞ {len(test_texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    
    results = analyzer.batch_analyze(test_texts, prompt_template=best_template)
    predictions = results['predictions']
    
    accuracy = accuracy_score(true_labels, predictions)
    
    from sklearn.metrics import classification_report, confusion_matrix
    
    print(f"\n–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"  Accuracy: {accuracy:.1%}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {results['avg_inference_time']:.3f} —Å–µ–∫")
    print(f"  –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {results['avg_response_length']:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print(f"\nConfusion Matrix:")
    cm = confusion_matrix(true_labels, predictions, labels=['–Ω–µ–≥–∞—Ç–∏–Ω—ã–π', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'])
    print(cm)
    
    print(f"\nClassification Report:")
    print(classification_report(true_labels, predictions))
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫:")
    errors = []
    for i, (text, pred, true) in enumerate(zip(test_texts, predictions, true_labels)):
        if pred != true:
            errors.append({
                'idx': i,
                'text_preview': text[:40] + "..." if len(text) > 40 else text,
                'predicted': pred,
                'true': true
            })
    
    print(f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)} –∏–∑ {len(test_texts)} ({len(errors)/len(test_texts):.1%})")
    
    if errors:
        print("\n–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫:")
        for error in errors[:3]:
            print(f"  –¢–µ–∫—Å—Ç: {error['text_preview']}")
            print(f"    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {error['predicted']}, –†–µ–∞–ª—å–Ω–æ–µ: {error['true']}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏
    print("\n" + "="*60)
    print("üÜö –°–†–ê–í–ù–ï–ù–ò–ï –° –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê–ú–ò")
    print("="*60)
    
    comparison_data = {
        '–ú–æ–¥–µ–ª—å': ['RuBERT-tiny2', 'RuBERT-base-sentiment', 'GPT-2 (–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è)'],
        'Accuracy': ['34%', '85%', f'{accuracy:.1%}'],
        '–í—Ä–µ–º—è (—Å–µ–∫)': ['0.002', '0.032', f'{results["avg_inference_time"]:.3f}'],
        '–¢–∏–ø': ['–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä', '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä', '–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è']
    }
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # –í—ã–≤–æ–¥—ã
    print("\n" + "="*60)
    print("üìã –í–´–í–û–î–´")
    print("="*60)
    
    print("\n1. –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å (GPT-2):")
    print(f"   - Accuracy: {accuracy:.1%}")
    print(f"   - –°–∫–æ—Ä–æ—Å—Ç—å: {results['avg_inference_time']:.3f} —Å–µ–∫ –Ω–∞ –ø—Ä–∏–º–µ—Ä")
    print(f"   - –õ—É—á—à–∏–π –ø—Ä–æ–º–ø—Ç: Few-shot")
    
    print("\n2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã vs –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ:")
    print("   - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏")
    print("   - –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤")
    print("   - Few-shot —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    
    print("\n3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("   - –î–ª—è production: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (85% accuracy)")
    print("   - –î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å few-shot –ø—Ä–æ–º–ø—Ç–∞–º–∏")
    print("   - –î–ª—è speed-critical –∑–∞–¥–∞—á: rubert-tiny2 (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π)")

def run_full_experiments():
    """–ü–æ–ª–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (–∑–∞–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ quick —Ä–∞–±–æ—Ç–∞–µ—Ç)"""
    print("–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")
    # ... (–∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–¥ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è, –Ω–æ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏)

if __name__ == "__main__":
    print("="*80)
    print("ü§ñ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –° –ì–ï–ù–ï–†–ê–¢–ò–í–ù–û–ô –ú–û–î–ï–õ–¨–Æ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò")
    print("="*80)
    
    run_quick_experiments()
    
    print("\n" + "="*80)
    print("‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´!")
    print(time.ctime())

    print("="*80)